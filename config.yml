language: zh

pipeline:
  - name: "JiebaTokenizer"
    # dictionary_path: "path/to/custom/dictionary/dir"
    # Flag to check whether to split intents
    intent_tokenization_flag: false
    # Symbol on which intent should be split
    intent_split_symbol: "_"
    # Regular expression to detect tokens
    # "token_pattern": None
  - name: "components.lm_featurizer.LanguageModelFeaturizer"
    model_name: "bert"
    model_weights: "bert-base-chinese"
    cache_dir: "./cache"
  - name: "RegexFeaturizer"
  - name: "LexicalSyntacticFeaturizer"
  - name: "CountVectorsFeaturizer"
  - name: "CountVectorsFeaturizer"
    analyzer: "char_wb"
    min_ngram: 1
    max_ngram: 7
  - name: "DIETClassifier"
    epochs: 100
  - name: "EntitySynonymMapper"

policies:
  - name: MemoizationPolicy
  - name: TEDPolicy
    epochs: 100
  - name: MappingPolicy
